{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7+2ZWPNstipHSS/5x+CxJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivytas0905/Alibaba_financial_report/blob/main/financial_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-openai pdfplumber numpy pypdf python-docx faiss-cpu sentence-transformers -U langchain-community python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaqPyuWS1Ugn",
        "outputId": "5c55d773-2714-4ad7-aa7a-bbfaa10711ee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.28)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.3.2)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.3.72)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.97.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.1)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.8)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.68->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Downloading pypdf-5.9.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "n-bXc_9g1I8K",
        "outputId": "fd74569e-4a2c-4d63-9d7b-33da741c1159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a910f0d6-cd8b-4a29-965c-3ee1c2f110fa\", \"alibaba_financial_data.json\", 1031)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON文件已生成并下载\n"
          ]
        }
      ],
      "source": [
        "import pdfplumber\n",
        "import re\n",
        "import json\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "pdf_path_en = \"/content/drive/MyDrive/alibaba_en.pdf\"\n",
        "pdf_path_cn = \"/content/drive/MyDrive/alibaba_cn.pdf\"\n",
        "\n",
        "\n",
        "data = {\n",
        "    \"financial_statements\": {\n",
        "        \"annual\": {\n",
        "            \"FY2024\": {\n",
        "                \"metrics\": {},\n",
        "                \"special_items\": {}\n",
        "            },\n",
        "            \"FY2025\": {\n",
        "                \"metrics\": {},\n",
        "                \"special_items\": {}\n",
        "            }\n",
        "        },\n",
        "        \"quarterly\": {}\n",
        "    },\n",
        "    \"business_segments\": {\n",
        "        \"FY2025\": {}\n",
        "    },\n",
        "    \"risk_factors\": [],\n",
        "    \"metadata\": {\n",
        "        \"currency\": \"RMB Mn\",\n",
        "        \"reporting_period\": \"Fiscal year ending March 31\"\n",
        "    }\n",
        " }\n",
        "\n",
        "TABLE_PATTERNS = {\n",
        "    \"revenue\": r\"Revenue\",\n",
        "    \"operating_income\": r\"Income from operations\",\n",
        "    \"net_income\": r\"Net income\",\n",
        "    \"adjusted_EBITA\": r\"Adjusted EBITA\"\n",
        "}\n",
        "#财务数据分配模式\n",
        "\n",
        "financial_pattern = {\n",
        "    \"total_revenue\": r\"Revenue[^\\d]*([\\d,.]+)\\s*(?:billion|B|USD|RMB)\",\n",
        "    \"net_profit\": r\"Net\\s*income[^\\d]*([\\d,.]+)\\s*(?:billion|B|USD|RMB)\",\n",
        "    \"operating_cash_flow\": r\"Net\\s*cash\\s*provided\\s*by\\s*operating\\s*activities[^\\d]*([\\d,.]+)\\s*(?:million|M|RMB|USD)\",\n",
        "    \"free_cash_flow\": r\"Free\\s*cash\\s*flow[^\\d]*([\\d,.]+)\\s*(?:million|M|RMB|USD)\",\n",
        "    \"adjusted_EBITA\": r\"Adjusted\\s*EBITA[^\\d]*([\\d,.]+)\\s*(?:billion|B|USD|RMB)\"\n",
        "}\n",
        "#分布数据匹配\n",
        "\n",
        "segment_patterns = {\n",
        "    \"Taobao_Tmall\": r\"Taobao and Tmall Group.*?Revenue.*?([\\d,.]+).*?Adjusted EBITA.*?([\\d,.\\-]+)\",\n",
        "    \"AIDC\": r\"Alibaba International Digital Commerce Group.*?Revenue.*?([\\d,.]+).*?Adjusted EBITA.*?([\\d,.\\-]+)\",\n",
        "    \"Cloud_Intelligence\": r\"Cloud Intelligence Group.*?Revenue.*?([\\d,.]+).*?Adjusted EBITA.*?([\\d,.\\-]+)\",\n",
        "    \"Cainiao\": r\"Cainiao Smart Logistics.*?Revenue.*?([\\d,.]+).*?Adjusted EBITA.*?([\\d,.\\-]+)\",\n",
        "    \"Local_Services\": r\"Local Services Group.*?Revenue.*?([\\d,.]+).*?Adjusted EBITA.*?([\\d,.\\-]+)\",\n",
        "    \"Digital_Media\": r\"Digital Media and Entertainment Group.*?Revenue.*?([\\d,.]+).*?Adjusted EBITA.*?([\\d,.\\-]+)\",\n",
        "    \"All_Others\": r\"All Others.*?Revenue.*?([\\d,.]+).*?Adjusted EBITA.*?([\\d,.\\-]+)\"\n",
        "}\n",
        "\n",
        "risk_keywords = [\"宏观经济 风险\", \"市场竞争 风险\", \"地缘政治\", \"数据安全\", \"网络安全\", \"法规监管\", \"合规\", \"监管风险\"]\n",
        "\n",
        "def extract_financial_tables(pdf_path):\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            tables = page.extract_tables()\n",
        "            for table in tables:\n",
        "                if len(table) > 5 and any(patt in str(table[2][0]) for patt in TABLE_PATTERNS.values()):\n",
        "                    process_financial_table(table)\n",
        "\n",
        "def process_financial_table(table):\n",
        "    # 确定列索引\n",
        "    col_map = {}\n",
        "    for i, header in enumerate(table[0]):\n",
        "        if \"2024\" in header: col_map[\"FY2024\"] = i\n",
        "        if \"2025\" in header: col_map[\"FY2025\"] = i\n",
        "\n",
        "    # 提取数据\n",
        "    for row in table[2:]:\n",
        "        if not row[0]: continue\n",
        "\n",
        "        for metric, pattern in TABLE_PATTERNS.items():\n",
        "            if re.search(pattern, row[0]):\n",
        "                for year, col in col_map.items():\n",
        "                    if row[col] and row[col].strip() not in (\"-\", \"\"):\n",
        "                        value = float(row[col].replace(\",\", \"\"))\n",
        "                        data[\"financial_statements\"][\"annual\"][year][\"metrics\"][metric] = value\n",
        "\n",
        "# 原有文本提取函数（优化版）\n",
        "def extract_text_data(pdf_path_en, pdf_path_cn):\n",
        "    # 英文财务数据\n",
        "    with pdfplumber.open(pdf_path_en) as pdf:\n",
        "        full_text_en = \"\\n\".join([p.extract_text(x_tolerance=2, y_tolerance=2)\n",
        "                                for p in pdf.pages if p.extract_text()])\n",
        "\n",
        "        # 补充提取表格未覆盖的数据\n",
        "        for year in [\"FY2024\", \"FY2025\"]:\n",
        "            year_text = re.search(f\"{year}.*?(?=(FY\\d|$))\", full_text_en, re.DOTALL)\n",
        "            if year_text:\n",
        "                for key, pattern in financial_pattern.items():\n",
        "                    match = re.search(pattern, year_text.group())\n",
        "                    if match:\n",
        "                        value = match.group(1).replace(\",\", \"\")\n",
        "                        data[\"financial_statements\"][\"annual\"][year][\"metrics\"][key] = float(value)\n",
        "\n",
        "        # 业务分部数据\n",
        "        for seg, pattern in segment_patterns.items():\n",
        "            match = re.search(pattern, full_text_en, re.DOTALL)\n",
        "            if match:\n",
        "                data[\"business_segments\"][\"FY2025\"][seg] = {\n",
        "                    \"revenue\": match.group(1).replace(\",\", \"\"),\n",
        "                    \"adjusted_EBITA\": match.group(2).replace(\",\", \"\")\n",
        "                }\n",
        "\n",
        "    # 中文风险因素\n",
        "    with pdfplumber.open(pdf_path_cn) as pdf:\n",
        "        full_text_cn = \"\\n\".join([p.extract_text() for p in pdf.pages if p.extract_text()])\n",
        "        sentences = re.split(r'(?<=[。！？；;])', full_text_cn)\n",
        "        data[\"risk_factors\"] = list(set(\n",
        "            sent.strip() for sent in sentences\n",
        "            if any(kw in sent for kw in risk_keywords) and len(sent) > 8\n",
        "        ))\n",
        "\n",
        "# 主执行流程\n",
        "def main():\n",
        "    # 优先提取表格数据\n",
        "    extract_financial_tables(pdf_path_en)\n",
        "\n",
        "    # 补充文本提取\n",
        "    extract_text_data(pdf_path_en, pdf_path_cn)\n",
        "\n",
        "    # 保存结果\n",
        "    json_path = \"/content/alibaba_financial_data.json\"\n",
        "    with open(json_path, 'w', encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # 下载文件\n",
        "    from google.colab import files\n",
        "    files.download(json_path)\n",
        "    print(\"JSON文件已生成并下载\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from dataclasses import dataclass\n",
        "from openai import OpenAI\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "@dataclass\n",
        "class TextChunk:\n",
        "    \"\"\"文本块数据结构\"\"\"\n",
        "    id: str\n",
        "    text: str\n",
        "    source: str  # 来源（如哪个财报、哪个章节）\n",
        "    metadata: Dict[str, Any]\n",
        "    embedding: np.ndarray = None\n",
        "class FinancialReportChunker:\n",
        "    \"\"\"财报文本智能切分器\"\"\"\n",
        "\n",
        "    def __init__(self, chunk_size: int = 1000, overlap: int = 200):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.overlap = overlap\n",
        "\n",
        "    def extract_text_from_json(self, json_data: Dict) -> Dict[str, str]:\n",
        "        \"\"\"从JSON财报数据中提取所有文本内容\"\"\"\n",
        "        extracted_texts = {}\n",
        "\n",
        "        def extract_recursive(data, path=\"root\"):\n",
        "            \"\"\"递归提取文本内容\"\"\"\n",
        "            if isinstance(data, dict):\n",
        "                for key, value in data.items():\n",
        "                    current_path = f\"{path}.{key}\"\n",
        "                    if isinstance(value, str) and len(value.strip()) > 10:\n",
        "                        # 过滤掉过短的文本\n",
        "                        extracted_texts[current_path] = value.strip()\n",
        "                    elif isinstance(value, (dict, list)):\n",
        "                        extract_recursive(value, current_path)\n",
        "            elif isinstance(data, list):\n",
        "                for idx, item in enumerate(data):\n",
        "                    current_path = f\"{path}[{idx}]\"\n",
        "                    extract_recursive(item, current_path)\n",
        "\n",
        "        extract_recursive(json_data)\n",
        "        return extracted_texts\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"清理文本内容\"\"\"\n",
        "        # 移除多余的空白字符\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        # 移除特殊字符但保留中文、英文、数字和基本标点\n",
        "        text = re.sub(r'[^\\u4e00-\\u9fff\\w\\s,.!?;:()（）【】\"\"''、。，！？；：-]', '', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def smart_chunk_text(self, text: str, source: str) -> List[TextChunk]:\n",
        "        \"\"\"智能文本切分\"\"\"\n",
        "        text = self.clean_text(text)\n",
        "        chunks = []\n",
        "\n",
        "        # 按段落分割\n",
        "        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
        "\n",
        "        current_chunk = \"\"\n",
        "        chunk_id = 0\n",
        "\n",
        "        for paragraph in paragraphs:\n",
        "            paragraph = paragraph.strip()\n",
        "            if not paragraph:\n",
        "                continue\n",
        "\n",
        "            # 如果当前块加上新段落不超过限制，则添加\n",
        "            if len(current_chunk) + len(paragraph) <= self.chunk_size:\n",
        "                current_chunk += paragraph + \"\\n\\n\"\n",
        "            else:\n",
        "                # 保存当前块\n",
        "                if current_chunk.strip():\n",
        "                    chunks.append(TextChunk(\n",
        "                        id=f\"{source}_chunk_{chunk_id}\",\n",
        "                        text=current_chunk.strip(),\n",
        "                        source=source,\n",
        "                        metadata={\n",
        "                            \"chunk_index\": chunk_id,\n",
        "                            \"length\": len(current_chunk.strip()),\n",
        "                            \"timestamp\": datetime.now().isoformat()\n",
        "                        }\n",
        "                    ))\n",
        "                    chunk_id += 1\n",
        "\n",
        "                # 开始新块，包含重叠内容\n",
        "                if len(current_chunk) > self.overlap:\n",
        "                    overlap_text = current_chunk[-self.overlap:]\n",
        "                    current_chunk = overlap_text + paragraph + \"\\n\\n\"\n",
        "                else:\n",
        "                    current_chunk = paragraph + \"\\n\\n\"\n",
        "\n",
        "        # 处理最后一个块\n",
        "        if current_chunk.strip():\n",
        "            chunks.append(TextChunk(\n",
        "                id=f\"{source}_chunk_{chunk_id}\",\n",
        "                text=current_chunk.strip(),\n",
        "                source=source,\n",
        "                metadata={\n",
        "                    \"chunk_index\": chunk_id,\n",
        "                    \"length\": len(current_chunk.strip()),\n",
        "                    \"timestamp\": datetime.now().isoformat()\n",
        "                }\n",
        "            ))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def process_financial_report(self, json_data: Dict, report_name: str) -> List[TextChunk]:\n",
        "        \"\"\"处理整个财报JSON数据\"\"\"\n",
        "        extracted_texts = self.extract_text_from_json(json_data)\n",
        "        all_chunks = []\n",
        "\n",
        "        for source_path, text in extracted_texts.items():\n",
        "            chunks = self.smart_chunk_text(text, f\"{report_name}_{source_path}\")\n",
        "            all_chunks.extend(chunks)\n",
        "\n",
        "        print(f\"从 {report_name} 中提取了 {len(all_chunks)} 个文本块\")\n",
        "        return all_chunks\n",
        "\n",
        "class EmbeddingService:\n",
        "    \"\"\"文本嵌入服务\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, model: str = \"text-embedding-ada-002\"):\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.model = model\n",
        "        self.embedding_dim = 1536  # ada-002的维度\n",
        "\n",
        "    def get_embeddings(self, texts: List[str], batch_size: int = 100) -> List[np.ndarray]:\n",
        "        \"\"\"批量获取文本嵌入\"\"\"\n",
        "        embeddings = []\n",
        "\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "            try:\n",
        "                response = self.client.embeddings.create(\n",
        "                    input=batch_texts,\n",
        "                    model=self.model\n",
        "                )\n",
        "                batch_embeddings = [np.array(data.embedding, dtype=np.float32)\n",
        "                                  for data in response.data]\n",
        "                embeddings.extend(batch_embeddings)\n",
        "                print(f\"已处理 {min(i + batch_size, len(texts))}/{len(texts)} 个文本\")\n",
        "            except Exception as e:\n",
        "                print(f\"批次 {i//batch_size + 1} 处理失败: {e}\")\n",
        "                # 使用零向量作为fallback\n",
        "                fallback_embeddings = [np.zeros(self.embedding_dim, dtype=np.float32)\n",
        "                                     for _ in batch_texts]\n",
        "                embeddings.extend(fallback_embeddings)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "class FAISSVectorDatabase:\n",
        "    \"\"\"FAISS向量数据库\"\"\"\n",
        "\n",
        "    def __init__(self, dimension: int = 1536):\n",
        "        self.dimension = dimension\n",
        "        self.index = faiss.IndexFlatIP(dimension)  # 使用内积相似度\n",
        "        self.chunks: List[TextChunk] = []\n",
        "        self.chunk_id_to_idx = {}\n",
        "\n",
        "    def add_chunks(self, chunks: List[TextChunk]):\n",
        "        \"\"\"添加文本块到数据库\"\"\"\n",
        "        if not chunks:\n",
        "            return\n",
        "\n",
        "        # 确保所有块都有嵌入向量\n",
        "        embeddings = []\n",
        "        valid_chunks = []\n",
        "\n",
        "        for chunk in chunks:\n",
        "            if chunk.embedding is not None:\n",
        "                embeddings.append(chunk.embedding)\n",
        "                valid_chunks.append(chunk)\n",
        "\n",
        "        if not embeddings:\n",
        "            print(\"警告: 没有有效的嵌入向量\")\n",
        "            return\n",
        "\n",
        "        # 标准化向量（对于内积相似度）\n",
        "        embeddings_array = np.array(embeddings, dtype=np.float32)\n",
        "        faiss.normalize_L2(embeddings_array)\n",
        "\n",
        "        # 添加到FAISS索引\n",
        "        start_idx = len(self.chunks)\n",
        "        self.index.add(embeddings_array)\n",
        "\n",
        "        # 更新元数据\n",
        "        for i, chunk in enumerate(valid_chunks):\n",
        "            idx = start_idx + i\n",
        "            self.chunks.append(chunk)\n",
        "            self.chunk_id_to_idx[chunk.id] = idx\n",
        "\n",
        "        print(f\"已添加 {len(valid_chunks)} 个向量到数据库，总计 {len(self.chunks)} 个\")\n",
        "\n",
        "    def search(self, query_embedding: np.ndarray, k: int = 5) -> List[Tuple[TextChunk, float]]:\n",
        "        \"\"\"语义检索\"\"\"\n",
        "        if len(self.chunks) == 0:\n",
        "            return []\n",
        "\n",
        "        # 标准化查询向量\n",
        "        query_embedding = query_embedding.reshape(1, -1).astype(np.float32)\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "\n",
        "        # 执行搜索\n",
        "        scores, indices = self.index.search(query_embedding, min(k, len(self.chunks)))\n",
        "\n",
        "        results = []\n",
        "        for score, idx in zip(scores[0], indices[0]):\n",
        "            if idx >= 0 and idx < len(self.chunks):\n",
        "                results.append((self.chunks[idx], float(score)))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def save(self, filepath: str):\n",
        "        \"\"\"保存数据库\"\"\"\n",
        "        # 保存FAISS索引\n",
        "        faiss.write_index(self.index, f\"{filepath}.faiss\")\n",
        "\n",
        "        # 保存元数据\n",
        "        metadata = {\n",
        "            'chunks': self.chunks,\n",
        "            'chunk_id_to_idx': self.chunk_id_to_idx,\n",
        "            'dimension': self.dimension\n",
        "        }\n",
        "        with open(f\"{filepath}.pkl\", 'wb') as f:\n",
        "            pickle.dump(metadata, f)\n",
        "\n",
        "        print(f\"数据库已保存到 {filepath}\")\n",
        "\n",
        "    def load(self, filepath: str):\n",
        "        \"\"\"加载数据库\"\"\"\n",
        "        # 加载FAISS索引\n",
        "        self.index = faiss.read_index(f\"{filepath}.faiss\")\n",
        "\n",
        "        # 加载元数据\n",
        "        with open(f\"{filepath}.pkl\", 'rb') as f:\n",
        "            metadata = pickle.load(f)\n",
        "\n",
        "        self.chunks = metadata['chunks']\n",
        "        self.chunk_id_to_idx = metadata['chunk_id_to_idx']\n",
        "        self.dimension = metadata['dimension']\n",
        "\n",
        "        print(f\"数据库已从 {filepath} 加载，包含 {len(self.chunks)} 个文本块\")\n",
        "\n",
        "class RAGAnalysisEngine:\n",
        "    \"\"\"RAG分析引擎主类\"\"\"\n",
        "\n",
        "    def __init__(self, openai_api_key: str, chunk_size: int = 1000, overlap: int = 200):\n",
        "        self.chunker = FinancialReportChunker(chunk_size, overlap)\n",
        "        self.embedding_service = EmbeddingService(openai_api_key)\n",
        "        self.vector_db = FAISSVectorDatabase()\n",
        "\n",
        "    def process_financial_reports(self, reports_data: List[Dict], report_names: List[str]):\n",
        "        \"\"\"处理多个财报\"\"\"\n",
        "        all_chunks = []\n",
        "\n",
        "        # 1. 文本切分\n",
        "        for report_data, report_name in zip(reports_data, report_names):\n",
        "            chunks = self.chunker.process_financial_report(report_data, report_name)\n",
        "            all_chunks.extend(chunks)\n",
        "\n",
        "        # 2. 向量化\n",
        "        texts = [chunk.text for chunk in all_chunks]\n",
        "        embeddings = self.embedding_service.get_embeddings(texts)\n",
        "\n",
        "        # 将嵌入向量分配给对应的文本块\n",
        "        for chunk, embedding in zip(all_chunks, embeddings):\n",
        "            chunk.embedding = embedding\n",
        "\n",
        "        # 3. 构建向量数据库\n",
        "        self.vector_db.add_chunks(all_chunks)\n",
        "\n",
        "        return len(all_chunks)\n",
        "\n",
        "    def semantic_search(self, query: str, k: int = 5) -> List[Tuple[TextChunk, float]]:\n",
        "        \"\"\"语义搜索\"\"\"\n",
        "        # 获取查询的嵌入向量\n",
        "        query_embedding = self.embedding_service.get_embeddings([query])[0]\n",
        "\n",
        "        # 执行搜索\n",
        "        results = self.vector_db.search(query_embedding, k)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def save_database(self, filepath: str):\n",
        "        \"\"\"保存数据库\"\"\"\n",
        "        self.vector_db.save(filepath)\n",
        "\n",
        "    def load_database(self, filepath: str):\n",
        "        \"\"\"加载数据库\"\"\"\n",
        "        self.vector_db.load(filepath)\n",
        "\n",
        "# 使用示例\n",
        "def main():\n",
        "    # 初始化RAG引擎\n",
        "    api_key = \"your-openai-api-key\"  # 替换为你的API密钥\n",
        "    rag_engine = RAGAnalysisEngine(api_key)\n",
        "\n",
        "    # 示例JSON数据（财报）\n",
        "    sample_financial_data = [\n",
        "        {\n",
        "            \"company\": \"示例公司A\",\n",
        "            \"period\": \"2024Q1\",\n",
        "            \"financial_summary\": {\n",
        "                \"revenue\": \"营业收入同比增长15%，达到50亿元人民币。主要增长来源于新产品线的推出和市场份额的扩大。\",\n",
        "                \"profit\": \"净利润为8亿元，同比增长12%。盈利能力持续改善，主要得益于成本控制和运营效率提升。\",\n",
        "                \"cash_flow\": \"经营活动现金流量净额为12亿元，现金流状况良好，为公司持续发展提供了充足的资金支持。\"\n",
        "            },\n",
        "            \"business_analysis\": {\n",
        "                \"market_position\": \"公司在行业中保持领先地位，市场占有率进一步提升至25%。\",\n",
        "                \"competitive_advantages\": \"技术创新能力强，拥有完善的产业链布局和强大的品牌影响力。\",\n",
        "                \"risk_factors\": \"面临原材料价格波动、汇率变化和市场竞争加剧等风险。\"\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"company\": \"示例公司B\",\n",
        "            \"period\": \"2024Q1\",\n",
        "            \"financial_summary\": {\n",
        "                \"revenue\": \"本季度营业收入30亿元，同比下降5%。主要受到市场需求疲软和竞争加剧的影响。\",\n",
        "                \"profit\": \"净利润3亿元，同比下降20%。盈利能力面临挑战，需要加强成本管控。\",\n",
        "                \"expenses\": \"销售费用和管理费用占营收比例上升，反映出运营效率有待提升。\"\n",
        "            },\n",
        "            \"strategic_initiatives\": {\n",
        "                \"digital_transformation\": \"加快数字化转型步伐，投资建设智能制造系统和数据分析平台。\",\n",
        "                \"market_expansion\": \"积极拓展海外市场，在东南亚地区新设立3个销售办事处。\"\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    report_names = [\"CompanyA_2024Q1\", \"CompanyB_2024Q1\"]\n",
        "\n",
        "    # 处理财报数据\n",
        "    print(\"开始处理财报数据...\")\n",
        "    total_chunks = rag_engine.process_financial_reports(sample_financial_data, report_names)\n",
        "    print(f\"总共处理了 {total_chunks} 个文本块\")\n",
        "\n",
        "    # 保存数据库\n",
        "    rag_engine.save_database(\"financial_reports_db\")\n",
        "\n",
        "    # 测试语义搜索\n",
        "    print(\"\\n测试语义搜索...\")\n",
        "    queries = [\n",
        "        \"营业收入增长情况\",\n",
        "        \"现金流状况\",\n",
        "        \"市场竞争和风险\",\n",
        "        \"数字化转型策略\"\n",
        "    ]\n",
        "\n",
        "    for query in queries:\n",
        "        print(f\"\\n查询: {query}\")\n",
        "        results = rag_engine.semantic_search(query, k=3)\n",
        "\n",
        "        for i, (chunk, score) in enumerate(results, 1):\n",
        "            print(f\"  结果 {i} (相似度: {score:.3f}):\")\n",
        "            print(f\"    来源: {chunk.source}\")\n",
        "            print(f\"    内容: {chunk.text[:200]}...\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu0GzPcD4Ao2",
        "outputId": "282a341f-5c66-4aa3-f7c7-52acec407ec8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始处理财报数据...\n",
            "从 CompanyA_2024Q1 中提取了 6 个文本块\n",
            "从 CompanyB_2024Q1 中提取了 5 个文本块\n",
            "批次 1 处理失败: Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope*******-key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
            "已添加 11 个向量到数据库，总计 11 个\n",
            "总共处理了 11 个文本块\n",
            "数据库已保存到 financial_reports_db\n",
            "\n",
            "测试语义搜索...\n",
            "\n",
            "查询: 营业收入增长情况\n",
            "批次 1 处理失败: Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope*******-key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
            "  结果 1 (相似度: 0.000):\n",
            "    来源: CompanyA_2024Q1_root.financial_summary.cash_flow\n",
            "    内容: 经营活动现金流量净额为12亿元，现金流状况良好，为公司持续发展提供了充足的资金支持。...\n",
            "  结果 2 (相似度: 0.000):\n",
            "    来源: CompanyA_2024Q1_root.financial_summary.profit\n",
            "    内容: 净利润为8亿元，同比增长12。盈利能力持续改善，主要得益于成本控制和运营效率提升。...\n",
            "  结果 3 (相似度: 0.000):\n",
            "    来源: CompanyA_2024Q1_root.financial_summary.revenue\n",
            "    内容: 营业收入同比增长15，达到50亿元人民币。主要增长来源于新产品线的推出和市场份额的扩大。...\n",
            "\n",
            "查询: 现金流状况\n",
            "批次 1 处理失败: Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope*******-key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
            "  结果 1 (相似度: 0.000):\n",
            "    来源: CompanyA_2024Q1_root.financial_summary.cash_flow\n",
            "    内容: 经营活动现金流量净额为12亿元，现金流状况良好，为公司持续发展提供了充足的资金支持。...\n",
            "  结果 2 (相似度: 0.000):\n",
            "    来源: CompanyA_2024Q1_root.financial_summary.profit\n",
            "    内容: 净利润为8亿元，同比增长12。盈利能力持续改善，主要得益于成本控制和运营效率提升。...\n",
            "  结果 3 (相似度: 0.000):\n",
            "    来源: CompanyA_2024Q1_root.financial_summary.revenue\n",
            "    内容: 营业收入同比增长15，达到50亿元人民币。主要增长来源于新产品线的推出和市场份额的扩大。...\n",
            "\n",
            "查询: 市场竞争和风险\n",
            "批次 1 处理失败: Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope*******-key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
            "  结果 1 (相似度: 0.000):\n",
            "    来源: CompanyA_2024Q1_root.financial_summary.cash_flow\n",
            "    内容: 经营活动现金流量净额为12亿元，现金流状况良好，为公司持续发展提供了充足的资金支持。...\n",
            "  结果 2 (相似度: 0.000):\n",
            "    来源: CompanyA_2024Q1_root.financial_summary.profit\n",
            "    内容: 净利润为8亿元，同比增长12。盈利能力持续改善，主要得益于成本控制和运营效率提升。...\n",
            "  结果 3 (相似度: 0.000):\n",
            "    来源: CompanyA_2024Q1_root.financial_summary.revenue\n",
            "    内容: 营业收入同比增长15，达到50亿元人民币。主要增长来源于新产品线的推出和市场份额的扩大。...\n",
            "\n",
            "查询: 数字化转型策略\n",
            "批次 1 处理失败: Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope*******-key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
            "  结果 1 (相似度: 0.000):\n",
            "    来源: CompanyA_2024Q1_root.financial_summary.cash_flow\n",
            "    内容: 经营活动现金流量净额为12亿元，现金流状况良好，为公司持续发展提供了充足的资金支持。...\n",
            "  结果 2 (相似度: 0.000):\n",
            "    来源: CompanyA_2024Q1_root.financial_summary.profit\n",
            "    内容: 净利润为8亿元，同比增长12。盈利能力持续改善，主要得益于成本控制和运营效率提升。...\n",
            "  结果 3 (相似度: 0.000):\n",
            "    来源: CompanyA_2024Q1_root.financial_summary.revenue\n",
            "    内容: 营业收入同比增长15，达到50亿元人民币。主要增长来源于新产品线的推出和市场份额的扩大。...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Optional, Dict\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from pypdf import PdfReader\n",
        "import docx\n",
        "\n",
        "# 导入 LangChain 组件\n",
        "try:\n",
        "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "except:\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "#加载 API Key\n",
        "load_dotenv(find_dotenv())\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "    except:\n",
        "        raise RuntimeError(\"未检测到 OPENAI_API_KEY，请在 .env 或 Colab Secrets 中设置。\")\n",
        "\n",
        "\n",
        "class FinancialReportRAG:\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_model=\"text-embedding-3-small\",\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        db_root: Path = Path(\"./vectorstores\")\n",
        "    ):\n",
        "        self.embedding_model = embedding_model\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.db_root = db_root\n",
        "\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=self.chunk_size,\n",
        "            chunk_overlap=self.chunk_overlap,\n",
        "            length_function=len\n",
        "        )\n",
        "        self.embeddings = OpenAIEmbeddings(model=self.embedding_model)\n",
        "\n",
        "    # -------- 文档加载 --------\n",
        "    def _read_pdf(self, file_path: Path) -> List[Dict]:\n",
        "        pages_data = []\n",
        "        with file_path.open(\"rb\") as f:\n",
        "            reader = PdfReader(f)\n",
        "            for i, page in enumerate(reader.pages, start=1):\n",
        "                page_text = page.extract_text() or \"\"\n",
        "                pages_data.append({\"page\": i, \"text\": page_text})\n",
        "        return pages_data\n",
        "\n",
        "    def _read_docx(self, file_path: Path) -> List[Dict]:\n",
        "        d = docx.Document(str(file_path))\n",
        "        content = \"\\n\".join(p.text for p in d.paragraphs if p.text)\n",
        "        return [{\"page\": 1, \"text\": content}]\n",
        "\n",
        "    def load_document(self, file_path: Path) -> List[Dict]:\n",
        "        if not file_path.exists():\n",
        "            raise FileNotFoundError(f\"文件不存在: {file_path}\")\n",
        "        suffix = file_path.suffix.lower()\n",
        "        if suffix == \".pdf\":\n",
        "            return self._read_pdf(file_path)\n",
        "        elif suffix == \".docx\":\n",
        "            return self._read_docx(file_path)\n",
        "        else:\n",
        "            raise ValueError(\"不支持的文件类型，仅支持 PDF 或 DOCX\")\n",
        "\n",
        "    # -------- Chunk 处理 --------\n",
        "    def chunk_document(self, pages_data: List[Dict]) -> List[Dict]:\n",
        "        chunks_with_meta = []\n",
        "        for page_data in pages_data:\n",
        "            chunks = self.text_splitter.split_text(page_data[\"text\"])\n",
        "            for chunk in chunks:\n",
        "                chunks_with_meta.append({\n",
        "                    \"content\": chunk,\n",
        "                    \"metadata\": {\"page\": page_data[\"page\"]}\n",
        "                })\n",
        "        return chunks_with_meta\n",
        "\n",
        "    # -------- 向量库构建 --------\n",
        "    def _db_path(self, db_name: str) -> Path:\n",
        "        return self.db_root / db_name\n",
        "\n",
        "    def build_or_update_db(self, file_path: Path, db_name: Optional[str] = None):\n",
        "        pages_data = self.load_document(file_path)\n",
        "        chunks = self.chunk_document(pages_data)\n",
        "\n",
        "        db_name = db_name or file_path.stem\n",
        "        db_dir = self._db_path(db_name)\n",
        "        self.db_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        texts = [c[\"content\"] for c in chunks]\n",
        "        metadatas = [c[\"metadata\"] for c in chunks]\n",
        "\n",
        "        if db_dir.exists():\n",
        "            db = FAISS.load_local(\n",
        "                str(db_dir),\n",
        "                self.embeddings,\n",
        "                allow_dangerous_deserialization=True\n",
        "            )\n",
        "            db.add_texts(texts, metadatas=metadatas)\n",
        "        else:\n",
        "            db = FAISS.from_texts(texts, self.embeddings, metadatas=metadatas)\n",
        "\n",
        "        db.save_local(str(db_dir))\n",
        "        print(f\"✅ 向量库已保存到 {db_dir}\")\n",
        "        return db\n",
        "\n",
        "    def load_db(self, db_name: str):\n",
        "        db_dir = self._db_path(db_name)\n",
        "        if not db_dir.exists():\n",
        "            raise FileNotFoundError(f\"向量库不存在: {db_dir}\")\n",
        "        return FAISS.load_local(\n",
        "            str(db_dir),\n",
        "            self.embeddings,\n",
        "            allow_dangerous_deserialization=True\n",
        "        )\n",
        "\n",
        "    # -------- RAG 检索增强 --------\n",
        "    def rag_search(self, db_name: str, question: str, keywords: Optional[List[str]] = None, k: int = 5):\n",
        "        db = self.load_db(db_name)\n",
        "        results = []\n",
        "\n",
        "        if keywords:\n",
        "            for kw in keywords:\n",
        "                results.extend(db.similarity_search(kw, k=k))\n",
        "        results.extend(db.similarity_search(question, k=k))\n",
        "\n",
        "        # 去重，合并结果\n",
        "        unique_results = []\n",
        "        seen_texts = set()\n",
        "        for r in results:\n",
        "            if r.page_content not in seen_texts:\n",
        "                seen_texts.add(r.page_content)\n",
        "                unique_results.append(r)\n",
        "\n",
        "        # 拼接上下文，附带页码\n",
        "        context = \"\"\n",
        "        for r in unique_results:\n",
        "            page = r.metadata.get(\"page\", \"?\")\n",
        "            context += f\"[Page {page}]\\n{r.page_content}\\n\\n\"\n",
        "        return context.strip()\n"
      ],
      "metadata": {
        "id": "Vx23pt63iBiS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "任务三"
      ],
      "metadata": {
        "id": "xQWZORtZl4u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from openai import OpenAI\n",
        "\n",
        "#任务二的 RAG 引擎类已定义并命名为 FinancialReportRAG\n",
        "rag = FinancialReportRAG()\n",
        "client = OpenAI()\n",
        "\n",
        "DB_NAME = \"alibaba_en\"\n",
        "PDF_PATH = Path(\"/content/drive/MyDrive/alibaba_en.pdf\")\n",
        "\n",
        "# Step 1: 确保向量库存在\n",
        "rag.build_or_update_db(PDF_PATH, db_name=DB_NAME)\n",
        "\n",
        "\n",
        "# 通用函数：RAG + GPT 问答\n",
        "# ------------------------\n",
        "def rag_answer(question: str, keywords=None, k=5, db_name=DB_NAME):\n",
        "    # 1) 从 RAG 检索上下文\n",
        "    context = rag.rag_search(db_name=db_name, question=question, keywords=keywords, k=k)\n",
        "\n",
        "    # 2) 构造 Prompt\n",
        "    prompt = f\"\"\"\n",
        "你是资深投资分析师，请基于以下财报原文回答问题，输出格式必须严格为：\n",
        "结论：\n",
        "理由：\n",
        "原文证据：（注明来源页码，统一格式 [Page X]）\n",
        "\n",
        "要求：\n",
        "- 结论部分简明扼要，投资经理可快速理解\n",
        "- 理由部分给出逻辑链条，解释为什么得出结论\n",
        "- 原文证据部分引用财报的原句或数据，且标注页码\n",
        "- 如果原文不足，请在证据部分明确标记为“原文未直接给出，以下为推断”\n",
        "\n",
        "-------------------\n",
        "财报内容：\n",
        "{context}\n",
        "-------------------\n",
        "问题：\n",
        "{question}\n",
        "答案：\n",
        "\"\"\"\n",
        "    # 3) 调用 LLM\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# 维度一：核心驱动力与护城河\n",
        "# ------------------------\n",
        "question_1 = \"\"\"\n",
        "根据财报中各业务分部的描述和数据，分析一下“阿里电商”作为核心业务，其护城河主要体现在哪些方面？\n",
        "请结合“全球拓展”、“用户基础”、“商家生态”、“平台技术”等角度，并从财报中找到原文证据。\n",
        "\"\"\"\n",
        "护城河_答案 = rag_answer(\n",
        "    question=question_1,\n",
        "    keywords=[\"阿里电商 护城河\", \"全球拓展\", \"用户基础\", \"商家生态\", \"平台技术\"]\n",
        ")\n",
        "print(\"\\n【维度一：核心驱动力与护城河】\\n\", 护城河_答案)\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# 维度二：商业模式与市场卡位\n",
        "# ------------------------\n",
        "question_2 = \"\"\"\n",
        "请用任务一提取的业务数据，计算出FY2025财年，云智能集团和阿里国际数字商业集团的收入分别占总收入的百分比。\n",
        "并结合财报中的关键信息，分析哪个分部是阿里当前最重要的“第二增长曲线”？理由是什么？\n",
        "\"\"\"\n",
        "商业模式_答案 = rag_answer(\n",
        "    question=question_2,\n",
        "    keywords=[\"云智能集团 收入\", \"阿里国际数字商业集团 收入\", \"FY2025 总收入\"]\n",
        ")\n",
        "print(\"\\n【维度二：商业模式与市场卡位】\\n\", 商业模式_答案)\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# 维度三：潜在风险与红旗警报\n",
        "# ------------------------\n",
        "question_3 = \"\"\"\n",
        "请从“风险”角度，提炼出管理层认为的关于“宏观经济与市场竞争”和“数据安全与法规监管”这两大类的主要风险描述。\n",
        "然后，请尝试回答：财报中的哪些财务数据（如收入增长放缓、利润率下降等）可能已经初步印证了“市场竞争”加剧的风险？\n",
        "\"\"\"\n",
        "风险_答案 = rag_answer(\n",
        "    question=question_3,\n",
        "    keywords=[\"宏观经济 风险\", \"市场竞争 风险\", \"数据安全 风险\", \"法规监管 风险\"]\n",
        ")\n",
        "print(\"\\n【维度三：潜在风险与红旗警报】\\n\", 风险_答案)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrjNX9FOlW94",
        "outputId": "0ba619b5-b377-4f65-809c-b6319432af69"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 向量库已保存到 vectorstores/alibaba_en\n",
            "\n",
            "【维度一：核心驱动力与护城河】\n",
            " 结论：\n",
            "阿里电商的护城河主要体现在全球拓展、用户基础、商家生态和平台技术四个方面。\n",
            "\n",
            "理由：\n",
            "1. **全球拓展**：阿里电商在国际市场上持续扩展，特别是在欧洲和海湾地区，通过多样化的产品和商业模式增强竞争优势。\n",
            "2. **用户基础**：阿里电商的高价值用户群体（如88VIP会员）持续增长，增强了平台的用户粘性和消费能力。\n",
            "3. **商家生态**：阿里电商致力于改善商家的运营环境，提供多种支持，促进商家的可持续发展，从而形成良好的商家生态。\n",
            "4. **平台技术**：阿里云在AI和云计算领域的技术领先地位，推动了平台的创新和用户体验的提升，进一步巩固了其市场领导地位。\n",
            "\n",
            "原文证据：\n",
            "- 全球拓展：“AIDC has a diverse geographical presence, with a consistent strategic focus on key regions such as select European markets and the Gulf Region.” [Page 10]\n",
            "- 用户基础：“The number of 88VIP members, our highest spending consumer group, continued to increase by double digits year-over-year, surpassing 50 million.” [Page 9]\n",
            "- 商家生态：“We remained focused on improving their operating environment and ensuring their sustainable development on our platform.” [Page 9]\n",
            "- 平台技术：“Alibaba Cloud was the only Chinese provider named an Emerging Leader in all four areas: Generative AI Model Providers, Generative AI Engineering, Generative AI Specialized Cloud Infrastructure, and AI Knowledge Management Apps/General Productivity.” [Page 11]\n",
            "\n",
            "【维度二：商业模式与市场卡位】\n",
            " 结论：云智能集团是阿里当前最重要的“第二增长曲线”。\n",
            "\n",
            "理由：根据财报数据，云智能集团在2025财年的收入为RMB30,127百万，而阿里总收入为RMB101,369百万。云智能集团的收入占总收入的29.7%（30,127 / 101,369 * 100），显示出其在阿里整体业务中的重要性。此外，云智能集团的收入增长率为18%，且AI相关产品的收入保持三位数的年增长率，表明其在未来增长潜力方面的强劲表现。相比之下，阿里国际数字商业集团的收入为RMB33,579百万，占总收入的32.5%（33,579 / 101,369 * 100），虽然其收入绝对值较高，但其增长率相对较低，且仍处于亏损状态。因此，云智能集团的快速增长和盈利能力提升使其成为阿里最重要的“第二增长曲线”。\n",
            "\n",
            "原文证据：\n",
            "1. 云智能集团收入为RMB30,127百万，占总收入的29.7%（计算得出）。\n",
            "2. 云智能集团收入增长18%和AI产品三位数增长的描述 [Page 11]。\n",
            "3. 阿里国际数字商业集团收入为RMB33,579百万，占总收入的32.5%（计算得出），但其调整后的EBITA为亏损 [Page 10]。\n",
            "\n",
            "【维度三：潜在风险与红旗警报】\n",
            " 结论：管理层认为宏观经济与市场竞争的风险主要体现在经济波动和竞争加剧上，而数据安全与法规监管的风险则涉及合规性和政策变化。\n",
            "\n",
            "理由：管理层指出，宏观经济和市场竞争的风险包括全球经济波动、竞争加剧以及地缘政治紧张局势等，这些因素可能会对公司的业务增长产生负面影响。同时，数据安全和法规监管的风险则与合规性和政策变化相关，可能影响公司的运营和战略执行。财务数据如收入增长放缓和利润率下降可能反映了市场竞争的加剧，表明公司在维持市场份额和盈利能力方面面临挑战。\n",
            "\n",
            "原文证据：\n",
            "1. 宏观经济与市场竞争风险描述：“fluctuations in general economic and business conditions in China and globally; uncertainties arising from competition among countries and geopolitical tensions” [Page 2]。\n",
            "2. 数据安全与法规监管风险描述：“risks related to strategic transactions; changes to our shareholder return initiatives” [Page 2]。\n",
            "3. 财务数据印证市场竞争加剧的风险：原文未直接给出，以下为推断。\n"
          ]
        }
      ]
    }
  ]
}